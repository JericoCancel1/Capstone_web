import os
import pickle
import numpy as np  # Ensure NumPy is imported
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Paths
MODEL_PATH = "G:/myenv/myenv2/siamese_model.h5"
DATASET_DIR = "G:/myenv/furnituredataset/tables/anchor"
OUTPUT_PICKLE_FILE = "dataset_embeddingss.pkl"

# Target size for image resizing
TARGET_SIZE = (28, 28)  # Update this if your model expects a different size

def load_siamese_model(model_path):
    """
    Load the Siamese model with the custom triplet loss function and extract the 4th layer as the embedding layer.
    
    :param model_path: Path to the trained Siamese model.
    :return: Embedding model.
    """
    # Register the custom loss function from TensorFlow Addons
    custom_objects = {"Addons>TripletSemiHardLoss": tfa.losses.TripletSemiHardLoss}

    # Load the full Siamese model
    siamese_model = load_model("siamese_model.h5", custom_objects=custom_objects)
    print("Siamese model loaded successfully.")
    
    # Extract the embedding layer
    embedding_layer = siamese_model.layers[3]  # Adjust this index if necessary
    print("Embedding layer extracted successfully.")
    return embedding_layer

def preprocess_image(image_path, target_size):
    """
    Preprocess the image for the model.
    
    :param image_path: Path to the image file.
    :param target_size: Tuple specifying the target size (height, width).
    :return: Preprocessed image as a NumPy array.
    """
    try:
        # Load the image and resize
        image = load_img(image_path, target_size=target_size)
        # Convert the image to a NumPy array and normalize
        image_array = img_to_array(image) / 255.0
        # Add a batch dimension
        return np.expand_dims(image_array, axis=0)
    except Exception as e:
        print(f"Error preprocessing image {image_path}: {e}")
        raise

def compute_and_store_embeddings():
    """
    Compute embeddings for images in the local dataset directory and store them in a pickle file
    with the correct format: {"embeddings": [...], "filenames": [...]}.
    """
    # Load the Siamese model and extract the embedding layer
    embedding_model = load_siamese_model(MODEL_PATH)

    # Ensure the dataset directory exists
    if not os.path.exists(DATASET_DIR):
        print(f"Dataset directory '{DATASET_DIR}' does not exist.")
        return

    # List all image files in the directory
    image_files = [f for f in os.listdir(DATASET_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

    if not image_files:
        print(f"No image files found in '{DATASET_DIR}'.")
        return

    # Initialize lists to store embeddings and filenames
    embeddings = []
    filenames = []

    # Compute embeddings for each image
    for image_name in image_files:
        image_path = os.path.join(DATASET_DIR, image_name)
        print(f"Processing: {image_path}")

        try:
            # Preprocess the image
            preprocessed_image = preprocess_image(image_path, TARGET_SIZE)

            # Compute embedding
            embedding = embedding_model.predict(preprocessed_image)

            # Append embedding and filename to the respective lists
            embeddings.append(embedding.flatten())  # Flatten to ensure consistent shape
            filenames.append(image_name)
        except Exception as e:
            print(f"Failed to process {image_name}: {e}")

    # Save embeddings and filenames to pickle file
    try:
        data = {"embeddings": np.array(embeddings), "filenames": filenames}
        with open(OUTPUT_PICKLE_FILE, "wb") as f:
            pickle.dump(data, f)
        print(f"Embeddings and filenames saved to {OUTPUT_PICKLE_FILE}")
    except Exception as e:
        print(f"Error saving pickle file: {e}")

# Run the script
if __name__ == "__main__":
    compute_and_store_embeddings()